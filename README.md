# ğŸ¤– AI-Powered Web Scraper with FAISS & Streamlit

This project is an **AI-powered web scraper** that extracts text from
websites, stores it in a **FAISS vector database**, and enables
**question-answering (Q&A)** using **Ollama LLM** with **Hugging Face
embeddings**.

------------------------------------------------------------------------

## ğŸš€ Features

-   ğŸŒ Scrape website text content (from `<p>` tags).
-   ğŸ” Store text chunks in a **FAISS vector database** for similarity
    search.
-   ğŸ§  Use **Hugging Face MiniLM embeddings** for text vectorization.
-   ğŸ¤– Answer user questions with context from scraped websites using
    **Mistral LLM** via Ollama.
-   ğŸ¨ Interactive **Streamlit Web UI**.

------------------------------------------------------------------------

## ğŸ› ï¸ Installation

1.  **Clone this repository**

``` bash
git clone https://github.com/MaddyRizvi/Web-Scrapper-AI-Agent.git
cd web-scraper-agent
```

2.  **Create & activate virtual environment (recommended)**

``` bash
python -m venv venv
source venv/bin/activate   # On Mac/Linux
venv\Scripts\activate    # On Windows
```

3.  **Install dependencies**

``` bash
pip install -r requirements.txt
```

------------------------------------------------------------------------

## ğŸ“¦ Requirements

Make sure the following Python packages are installed:

-   `requests`
-   `streamlit`
-   `faiss-cpu`
-   `numpy`
-   `beautifulsoup4`
-   `langchain`
-   `langchain-community`
-   `langchain-ollama`
-   `sentence-transformers`

Create a `requirements.txt` file with:

``` txt
requests
streamlit
faiss-cpu
numpy
beautifulsoup4
langchain
langchain-community
langchain-ollama
sentence-transformers
```

------------------------------------------------------------------------

## â–¶ï¸ Usage

Run the Streamlit app:

``` bash
streamlit run scraper_app.py
```

Then open the URL provided by Streamlit (default:
`http://localhost:8501`) in your browser.

------------------------------------------------------------------------

## ğŸ’¡ How It Works

1.  User enters a **website URL**.
2.  Website text (`<p>` tags) is **scraped** and stored in FAISS.
3.  Text is **split into chunks** and embedded using Hugging Face MiniLM
    embeddings.
4.  User enters a **question** â†’ FAISS retrieves relevant context.
5.  **Mistral LLM** (via Ollama) generates an **AI-powered answer**.

------------------------------------------------------------------------

## ğŸ“ Example

-   Input: `https://example.com`
-   Question: `"What is this website about?"`
-   Output: ğŸ¤– AI generates an answer based on the scraped content.

------------------------------------------------------------------------

## âš¡ Notes

-   FAISS stores chunks in-memory only (not persistent).
-   Text is limited to **5000 characters** per site to avoid overload.
-   Ensure **Ollama** is installed and running with the `mistral` model.

------------------------------------------------------------------------

## ğŸ“œ License

This project is licensed under the **MIT License**.

------------------------------------------------------------------------

ğŸ‘¨â€ğŸ’» Developed with â¤ï¸ using **Python, FAISS, HuggingFace & Streamlit**.
